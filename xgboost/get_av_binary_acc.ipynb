{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get average binary accuracy\n",
    "\n",
    "Look through all the metrics of the XGBoost binary models, and calculate the average accuracy and mcc (so we can quote this in the writeup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "metrics = [metric for metric in os.listdir('figs') if 'metrics_xgboost_' in metric and metric != 'metrics_xgboost_.csv'] # avoiding the multiclass one\n",
    "import pandas as pd\n",
    "metrics = [pd.read_csv('figs/' + metric, index_col=0) for metric in metrics]\n",
    "\n",
    "\n",
    "def get_vals(metrics):\n",
    "    accs = []\n",
    "    mccs = []\n",
    "    for i in range(len(metrics)):\n",
    "        accs.append(np.float(metrics[i].loc['Accuracy'].values[0]))\n",
    "        mccs.append(np.float(metrics[i].loc['mcc'].values[0]))\n",
    "    return accs, mccs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.981651376146789,\n",
       " 0.9910313901345291,\n",
       " 1.0,\n",
       " 0.9607843137254902,\n",
       " 0.993103448275862,\n",
       " 1.0,\n",
       " 0.9629629629629629,\n",
       " 0.9907407407407407,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9640287769784173,\n",
       " 0.993006993006993,\n",
       " 1.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.7672543550107871,\n",
       " 0.9526518804404888,\n",
       " 1.0,\n",
       " 0.6925256939166184,\n",
       " 0.9571501946318893,\n",
       " 1.0,\n",
       " 0.8491201810548652,\n",
       " 0.9557518184453213,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8131737673374138,\n",
       " 0.9599107101332767,\n",
       " 1.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc of  13  results is:  0.9874853847670603\n",
      "Average mcc of  13  results is:  0.919041430843897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "accs, mccs = get_vals(metrics)\n",
    "accs\n",
    "mccs\n",
    "\n",
    "av_acc = np.mean(accs)\n",
    "av_mcc = np.mean(mccs)\n",
    "print(\"Average acc of \" , len(metrics) , \" results is: \", av_acc)\n",
    "print(\"Average mcc of \" , len(metrics) , \" results is: \", av_mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
