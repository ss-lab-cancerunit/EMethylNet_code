{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings:\n",
    "\n",
    "cancer_type = '' # '' for multiclass\n",
    "classifier_type = 'XGBoost'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting values and diagnoses from: \n",
      "/Tank/methylation-patterns-code/methylation-patterns-izzy/data_preprocessing/dataset/pandas/m_values/TCGA-BLCA.csv\n",
      "/Tank/methylation-patterns-code/methylation-patterns-izzy/data_preprocessing/dataset/pandas/diagnoses/TCGA-BLCA.csv\n",
      "m_value and diagnoses shapes:\n",
      "(278118, 436)\n",
      "(436,)\n",
      "m values train, m values test, diagnoses train, diagnoses test shapes:\n",
      "(327, 278118) (109, 278118) (327,) (109,)\n"
     ]
    }
   ],
   "source": [
    "# adding this path so we can import get_train_and_test\n",
    "import sys\n",
    "path = '../'\n",
    "sys.path.append(path)\n",
    "from get_train_and_test import get_train_and_test\n",
    "root_path = path\n",
    "\n",
    "seed = 42 # using a seed for splitting up the train and test data \n",
    "\n",
    "m_values_train, m_values_test, diagnoses_train, diagnoses_test = get_train_and_test(cancer_type, use_small=False, root_path = root_path, model_path = root_path + '/xgboost/', model_type = classifier_type + '_' + cancer_type, seed = seed)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "bst = xgb.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           learning_rate=0.1, max_delta_step=0,\n",
       "                                           max_depth=3, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=100,\n",
       "                                           n_jobs=1, nthread=None,\n",
       "                                           objective='binary:logistic',\n",
       "                                           random_state=0, reg_alpha=0,\n",
       "                                           reg_lambda=1, scale_pos_weight=1,\n",
       "                                           seed=None, silent=None, subsample=1,\n",
       "                                           verbosity=1),\n",
       "                   iid='warn', n_iter=5, n_jobs=None,\n",
       "                   param_distributions={'learning_rate': [0.189],\n",
       "                                        'max_depth': [10],\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f752675a690>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>458</td>\n",
       "      <td>0.975535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>436</td>\n",
       "      <td>0.975535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>307</td>\n",
       "      <td>0.975535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>248</td>\n",
       "      <td>0.975535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>218</td>\n",
       "      <td>0.975535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  max_depth  n_estimators     score\n",
       "0          0.189         10           458  0.975535\n",
       "1          0.189         10           436  0.975535\n",
       "2          0.189         10           307  0.975535\n",
       "3          0.189         10           248  0.975535\n",
       "4          0.189         10           218  0.975535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  max_depth  n_estimators     score\n",
      "0          0.189         10           458  0.975535\n",
      "1          0.189         10           436  0.975535\n",
      "2          0.189         10           307  0.975535\n",
      "3          0.189         10           248  0.975535\n",
      "4          0.189         10           218  0.975535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/xgboost_model_multiclass_best_params_{\"learning_rate\": 0.189, \"max_depth\": 10, \"n_estimators\": 458}.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# binary version:\n",
    "# to_try = {'max_depth': randint(3, 13),\n",
    "#           'n_estimators': randint(50, 500),\n",
    "#           'learning_rate':uniform(0.001,0.5)}\n",
    "\n",
    "to_try = {'max_depth': [3, 2],\n",
    "         'n_estimators': [500, 800, 1200],\n",
    "         'subsample': [0.5, 0.25],\n",
    "          'colsample_bytree': [0.5, 0.25, 0.125],\n",
    "          'objective':'binary:logistic',\n",
    "          'learning_rate':0.189\n",
    "         }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(bst, to_try, n_iter=5, scoring='f1_micro', cv=3)\n",
    "random_search.fit(m_values_train, diagnoses_train)\n",
    "\n",
    "random_search_results = random_search.cv_results_\n",
    "random_search_best_estimator = random_search.best_estimator_\n",
    "\n",
    "# showing parameters and scores\n",
    "scores = random_search.cv_results_['mean_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "import pandas as pd\n",
    "pd_results = pd.DataFrame(params)\n",
    "pd_results[\"score\"] = scores\n",
    "pd_results.sort_values(by=\"score\", ascending = False)\n",
    "\n",
    "print(pd_results)\n",
    "\n",
    "pd_results.to_csv('pd_results.csv')\n",
    "\n",
    "# saving best model\n",
    "params = random_search.best_params_\n",
    "import json\n",
    "params = json.dumps(params) # get string rep of dictionary\n",
    "import joblib # joblib is apparently more efficient than pickle functions for model saving (see https://scikit-learn.org/stable/modules/model_persistence.html)\n",
    "joblib.dump(random_search_best_estimator, 'saved_models/xgboost_model_multiclass_best_params_'+params+'.pkl')\n",
    "booster = random_search_best_estimator.get_booster()\n",
    "booster.dump_model('saved_models/xgboost_trees_multiclass_best_params_'+params+'.txt')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This was run on binary classification, for cancer type BLCA.\n",
    "Results from:\n",
    "to_try = {'max_depth': randint(3, 13),\n",
    "          'n_estimators': randint(50, 500),\n",
    "          'learning_rate':uniform(0.001,0.5)}\n",
    "\n",
    " \tlearning_rate \tmax_depth \tn_estimators \tscore\n",
    "3 \t0.189816 \t8 \t255 \t0.975535\n",
    "12 \t0.206255 \t9 \t378 \t0.975535\n",
    "10 \t0.328453 \t11 \t174 \t0.972477\n",
    "1 \t0.208564 \t9 \t392 \t0.972477\n",
    "18 \t0.481306 \t5 \t130 \t0.972477\n",
    "17 \t0.288161 \t11 \t226 \t0.972477\n",
    "15 \t0.107656 \t4 \t253 \t0.972477\n",
    "14 \t0.487466 \t9 \t235 \t0.972477\n",
    "11 \t0.075950 \t10 \t483 \t0.972477\n",
    "19 \t0.183306 \t11 \t151 \t0.972477\n",
    "9 \t0.061807 \t12 \t382 \t0.972477\n",
    "8 \t0.051273 \t11 \t246 \t0.972477\n",
    "6 \t0.143621 \t10 \t178 \t0.972477\n",
    "5 \t0.248130 \t3 \t188 \t0.972477\n",
    "2 \t0.476723 \t10 \t170 \t0.972477\n",
    "7 \t0.275979 \t10 \t488 \t0.969419\n",
    "13 \t0.021426 \t7 \t124 \t0.969419\n",
    "16 \t0.007922 \t3 \t380 \t0.969419\n",
    "4 \t0.302831 \t3 \t212 \t0.969419\n",
    "0 \t0.001891 \t6 \t187 \t0.948012\n",
    "\n",
    "Will now set max_depth = 10 and lr = 0.189, and vary the max depth. I think it will be better higher.\n",
    "\n",
    "Turns out that it doesn't make a difference:\n",
    "   learning_rate  max_depth  n_estimators     score\n",
    "0          0.189         10           458  0.975535\n",
    "1          0.189         10           436  0.975535\n",
    "2          0.189         10           307  0.975535\n",
    "3          0.189         10           248  0.975535\n",
    "4          0.189         10           218  0.975535\n",
    "\n",
    "\n",
    "So I will go with more trees because that might help give me more features.\n",
    "\n",
    "Final parameter decisions:\n",
    "params = {'max_depth': 10,\n",
    "          'n_estimators': 450,\n",
    "          'learning_rate':0.189}\n",
    "          \n",
    "\n",
    "For the multiclass xgboost, it takes ages to train so a normal hyperparameter search would last weeks.\n",
    "I tried it with \n",
    "params = {'max_depth': 10,\n",
    "          'n_estimators': 500,\n",
    "          'learning_rate':0.38}\n",
    "and got good (test set) results: Accuracy = 0.9768, mcc = 0.9748.\n",
    "\n",
    "I then tried it with the optimised binary parameters:\n",
    "params = {'max_depth': 10,\n",
    "          'n_estimators': 450,\n",
    "          'learning_rate':0.189}\n",
    "and got very slightly better (test set) results: Accuracy=0.9787, mcc=0.9769.\n",
    "\n",
    "Now I will try following Micheal's advice - use stubs of trees (max_depth=3 or 2), with colsample_bytree = 0.7 ish, and using gamma.\n",
    "These results were very good. Then I did a hyperparameter search around this hyperparameter space:\n",
    "\n",
    "Results:\n",
    "\n",
    "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
    "\n",
    "\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.973, total=691.4min\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.978, total=642.0min\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.979, total=645.2min\n",
    "= 0.9766\n",
    "\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.25, score=0.970, total=801.5min\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.25, score=0.978, total=802.1min\n",
    "[CV]  subsample=0.5, objective=binary:logistic, n_estimators=800, max_depth=2, learning_rate=0.189, colsample_bytree=0.25, score=0.976, total=842.1min\n",
    "= 0.97466\n",
    "\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.971, total=914.1min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.974, total=898.6min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=2, learning_rate=0.189, colsample_bytree=0.125, score=0.971, total=898.1min\n",
    "= 0.972\n",
    "\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.968, total=1482.9min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.973, total=1905.5min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=1200, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.973, total=1785.2min\n",
    "= 0.971333\n",
    "\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=500, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.969, total=686.0min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=500, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.972, total=696.0min\n",
    "[CV]  subsample=0.25, objective=binary:logistic, n_estimators=500, max_depth=3, learning_rate=0.189, colsample_bytree=0.5, score=0.973, total=738.8min\n",
    "= 0.971333\n",
    "\n",
    "The first one looks good - it has the best score, and doesn't take too long. Note the scores here are not comparable to the scores of trained models, as these models are only trained on 2/3 of the data.\n",
    "\n",
    "params = {\n",
    "    'subsample': 0.5, \n",
    "    'objective': binary:logistic, \n",
    "    'n_estimators' : 800, \n",
    "    'max_depth' : 2, \n",
    "    'learning_rate' : 0.189, \n",
    "    'colsample_bytree' : 0.125\n",
    "}\n",
    "\n",
    "BUT then I tried these similar params (just small changes to max_depth and colsample_bytree) and this seemed to perform the best:\n",
    "\n",
    "Final multiclass params:\n",
    "params = {\n",
    "    'subsample': 0.5, \n",
    "    'objective': binary:logistic, \n",
    "    'n_estimators' : 800, \n",
    "    'max_depth' : 3, \n",
    "    'learning_rate' : 0.189, \n",
    "    'colsample_bytree' : 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
